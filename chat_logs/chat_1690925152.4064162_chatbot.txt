Yes, few-shot learning could be a good approach for this task, especially if you have a limited amount of annotated data. In few-shot learning, the model is trained to learn a task from a small number of examples (the "shots"). This is a common approach in NLP and can be very effective for tasks like text extraction.

For this task, you would prepare your dataset in the same way as before, by annotating a small number of examples with the start and end indices of the forward-looking statements. The difference is that you would only need a few examples to train your model.

Here's a simplified example of how you might use the Hugging Face's Transformers library for few-shot learning:

```python
from transformers import GPTNeoForCausalLM, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-2.7B')

# Prepare your prompt and examples
prompt = "In the following text, identify the forward-looking statements:"
examples = [
    ("The market has been volatile, but we expect it to stabilize in the coming months.", "we expect it to stabilize in the coming months"),
    ("Our fund has performed well this year and we anticipate continued growth.", "we anticipate continued growth"),
    # Add more examples
]

# Concatenate your prompt and examples into a single string
input_text = prompt + "\n\n" + "\n\n".join(f"Text: {text}\nStatement: {statement}" for text, statement in examples)

# Tokenize your input text
inputs = tokenizer(input_text, return_tensors='pt')

# Generate a response from the model
outputs = model.generate(**inputs)

# Decode the response
response = tokenizer.decode(outputs[0])

# The response should contain the forward-looking statements from your new text
```

This is a very simplified example and the actual code may vary depending on your specific needs. I recommend checking out the Hugging Face's Transformers documentation and tutorials for more information.

As for the ChatGPT API, it could be used for this task, but it's important to note that it has a maximum token limit (currently 4096 tokens for gpt-3.5-turbo). If your filings are longer than this, you would need to split them into smaller chunks before processing them with the API.