Absolutely, Cinder. Training an NLP model to extract specific sections from unstructured text like SEC filings involves several steps. Here's a simplified overview:

1. **Data Preparation**: Start by gathering a dataset of SEC filings. You've already started this by downloading the .htm files from the SEC Edgar. You'll need to convert these into a text format that your NLP model can process.

2. **Annotation**: Identify and label the sections of interest in your dataset. This could be done manually or semi-automatically with the help of some basic NLP techniques. This step is crucial as it forms the basis for training your model.

3. **Model Selection**: Choose an NLP model that's suitable for your task. For text extraction tasks like yours, models based on the Transformer architecture, such as BERT or GPT, are often used. There are many pre-trained models available that you can fine-tune on your task.

4. **Training**: Fine-tune your chosen model on your annotated dataset. This involves running the model on your data and adjusting the model's parameters to minimize the difference between the model's predictions and the actual labels.

5. **Evaluation**: After training, evaluate your model's performance on a separate test set to ensure it can generalize to unseen data.

6. **Deployment**: Once you're satisfied with your model's performance, you can use it to extract the relevant sections from new SEC filings.

This is a high-level overview and each step involves more detailed work. There are many resources available online to help you get started with NLP and model training. Libraries like Hugging Face's Transformers and spaCy provide pre-trained models and tools that can be very helpful.

Remember, this process can be complex and time-consuming, especially if you're new to NLP. Don't hesitate to seek help or use available tools and resources. Would you like more detailed information on any of these steps?